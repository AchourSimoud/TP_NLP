{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error,accuracy_score,confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim\n",
    "import contractions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data/yelp-text-by-stars.csv\",delimiter=\";\" ,encoding=\"iso-8859-1\")\n",
    "X=data.iloc[:,1]\n",
    "Y=data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.patches.StepPatch at 0x2437b800710>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y.describe\n",
    "counts, bins = np.histogram(Y)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des textes\n",
    "def clean(data):\n",
    "    # Transformation en minuscule\n",
    "    X = data.str.lower()\n",
    "    \n",
    "    # Reconstruction des contractions type \"dont\" \"they're\"\n",
    "    X = X.apply(lambda x: contractions.fix(x))\n",
    "    \n",
    "    # Suppression des liens et des caractères spéciaux\n",
    "    X = X.replace(to_replace=r'^https?:\\/\\/.*[\\r\\n]*|[\\'.,!?$()*%@\\\"-_]', value='', regex=True)\n",
    "    \n",
    "    # Initialisation du stemmer\n",
    "    stemmer = LancasterStemmer()\n",
    "    \n",
    "    # Application de la stemmatisation sur chaque texte\n",
    "    stems = [' '.join([stemmer.stem(w) for w in text.split()]) for text in X]\n",
    "    \n",
    "    return pd.Series(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyages des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=clean(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    board my engl mastiff her ov new year they ar ...\n",
       "1    anoth cas of the emp new cloth someon of the a...\n",
       "2    cam on valentin day night hav prebought ticket...\n",
       "3    nd tim eat her todayst tim was gre but now i d...\n",
       "4    allegy is a disast their far ar cheap but not ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des ensemble d'entrainnement et de test\n",
    "corpus_train,corpus_test,y_train,y_test=train_test_split(corpus,Y,test_size=0.33,shuffle=True,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'execution des algorithmes de prediction\n",
    "def run_models (X_train,Y_train,X_test,Y_test,algos):\n",
    "    for algo_name in algos:\n",
    "        model=algos[algo_name]\n",
    "        model.fit(X_train,Y_train)\n",
    "        prediction=model.predict(X_test)\n",
    "        prediction[prediction<1]=1\n",
    "        prediction[prediction>5]=5\n",
    "        MAE=mean_absolute_error(Y_test,prediction)\n",
    "        ACC=accuracy_score(Y_test,np.round(prediction))\n",
    "        \n",
    "        print('################## {0} #############'.format(algo_name))\n",
    "        print('MAE = {0:.3f}, Accuracy ={1:.3f}'.format(MAE,ACC))\n",
    "        display(confusion_matrix(Y_test,np.round(prediction)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "'RF' : RandomForestRegressor(n_estimators=50,random_state=1,n_jobs=-1),\n",
    " 'KNN' : KNeighborsRegressor(n_neighbors=5,n_jobs=-1,metric='cosine'),\n",
    "  'MLP' : MLPRegressor(hidden_layer_sizes=(20,10),max_iter=200,random_state=1,alpha=0.001)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF=TfidfVectorizer(max_features=1000,stop_words='english')\n",
    "TFIDF.fit(corpus_train)\n",
    "corpus_train_tfidf=TFIDF.transform(corpus_train)\n",
    "corpus_test_tfidf=TFIDF.transform(corpus_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24252485, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32965217, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.36457438,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.27118916, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.28067726, 0.        ,\n",
       "        0.        , 0.27979412, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25096929,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.17018928, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.14223893,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18770007,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23857574, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.2961496 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.13386049, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.26134301, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30122026, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour afficher le premier texte transformé par TFIDF\n",
    "corpus_train_tfidf[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nic': 559,\n",
       " 'shop': 775,\n",
       " 'easy': 266,\n",
       " 'appoint': 37,\n",
       " 'profess': 672,\n",
       " 'friend': 345,\n",
       " 'act': 5,\n",
       " 'gre': 369,\n",
       " 'convers': 179,\n",
       " 'dur': 263,\n",
       " 'cut': 208,\n",
       " 'styl': 850,\n",
       " 'perfect': 617,\n",
       " 'serv': 768,\n",
       " 'excel': 289,\n",
       " 'today': 898,\n",
       " 'tim': 893,\n",
       " 'und': 928,\n",
       " 'awesom': 52,\n",
       " 'expery': 293,\n",
       " 'definit': 220,\n",
       " 'wil': 974,\n",
       " 'going': 364,\n",
       " 'addit': 7,\n",
       " 'bbq': 63,\n",
       " 'favorit': 313,\n",
       " 'food': 333,\n",
       " 'sint': 785,\n",
       " 'liv': 481,\n",
       " 'happy': 386,\n",
       " 'try': 921,\n",
       " 'new': 558,\n",
       " 'spot': 825,\n",
       " 'op': 580,\n",
       " 'stand': 829,\n",
       " 'loc': 483,\n",
       " 'let': 470,\n",
       " 'hid': 399,\n",
       " 'ar': 39,\n",
       " 'park': 605,\n",
       " 'lot': 489,\n",
       " 'warm': 952,\n",
       " 'sid': 779,\n",
       " 'mex': 525,\n",
       " 'resta': 713,\n",
       " 'insid': 433,\n",
       " 'fant': 307,\n",
       " 'cle': 152,\n",
       " 'simpl': 782,\n",
       " 'ther': 883,\n",
       " 'long': 485,\n",
       " 'tabl': 863,\n",
       " 'thos': 889,\n",
       " 'want': 950,\n",
       " 'enjoy': 276,\n",
       " 'did': 233,\n",
       " 'hom': 404,\n",
       " 'husband': 418,\n",
       " 'select': 763,\n",
       " 'meat': 513,\n",
       " 'spec': 816,\n",
       " 'com': 161,\n",
       " 'pric': 665,\n",
       " 'mor': 542,\n",
       " 'reason': 692,\n",
       " 'qual': 678,\n",
       " 'includ': 427,\n",
       " 'ad': 6,\n",
       " 'corn': 183,\n",
       " 'shar': 773,\n",
       " 'al': 18,\n",
       " 'tri': 916,\n",
       " 'wer': 967,\n",
       " 'delicy': 222,\n",
       " 'pul': 676,\n",
       " 'pork': 648,\n",
       " 'sup': 857,\n",
       " 'sweet': 862,\n",
       " 'sauc': 748,\n",
       " 'rib': 717,\n",
       " 'amaz': 22,\n",
       " 'wel': 964,\n",
       " 'tend': 876,\n",
       " 'just': 445,\n",
       " 'right': 722,\n",
       " 'bon': 87,\n",
       " 'flav': 327,\n",
       " 'hot': 411,\n",
       " 'expect': 291,\n",
       " 'plu': 640,\n",
       " 'bak': 57,\n",
       " 'bean': 64,\n",
       " 'ton': 903,\n",
       " 'potato': 654,\n",
       " 'salad': 737,\n",
       " 'pleas': 638,\n",
       " 'tast': 869,\n",
       " 'dress': 256,\n",
       " 'problem': 669,\n",
       " 'chees': 139,\n",
       " 'hold': 403,\n",
       " 'fri': 343,\n",
       " 'light': 474,\n",
       " 'season': 756,\n",
       " 'bread': 96,\n",
       " 'mad': 498,\n",
       " 'good': 366,\n",
       " 'pie': 628,\n",
       " 'cook': 180,\n",
       " 'green': 371,\n",
       " 'origin': 586,\n",
       " 'ord': 584,\n",
       " 'point': 642,\n",
       " 'apolog': 32,\n",
       " 'said': 735,\n",
       " 'apprecy': 38,\n",
       " 'hav': 389,\n",
       " 'heard': 393,\n",
       " 'comply': 171,\n",
       " 'custom': 207,\n",
       " 'stor': 843,\n",
       " 'visit': 944,\n",
       " 'absolv': 2,\n",
       " 'lov': 492,\n",
       " 'thi': 885,\n",
       " 'fin': 321,\n",
       " 'thing': 886,\n",
       " 'lif': 473,\n",
       " 'mind': 530,\n",
       " 'drop': 259,\n",
       " 'quick': 680,\n",
       " 'look': 486,\n",
       " 'sho': 774,\n",
       " 'rath': 688,\n",
       " 'spend': 817,\n",
       " 'money': 539,\n",
       " 'item': 438,\n",
       " 'year': 994,\n",
       " 'month': 541,\n",
       " 'mac': 496,\n",
       " 'spent': 818,\n",
       " 'hour': 413,\n",
       " 'don': 250,\n",
       " 'alway': 21,\n",
       " 'mak': 501,\n",
       " 'sur': 859,\n",
       " 'stock': 841,\n",
       " 'sal': 736,\n",
       " 'someth': 803,\n",
       " 'tru': 918,\n",
       " 'bag': 56,\n",
       " 'pair': 601,\n",
       " 'real': 691,\n",
       " 'afford': 10,\n",
       " 'everyon': 285,\n",
       " 'ont': 579,\n",
       " 'rol': 727,\n",
       " 'round': 729,\n",
       " 'howev': 415,\n",
       " 'unfortun': 930,\n",
       " 'beauty': 66,\n",
       " 'despit': 231,\n",
       " 'abl': 0,\n",
       " 'dec': 217,\n",
       " 'think': 887,\n",
       " 'fair': 303,\n",
       " 'rang': 685,\n",
       " 'stil': 840,\n",
       " 'th': 880,\n",
       " 'flo': 330,\n",
       " 'cocktail': 157,\n",
       " 'sush': 861,\n",
       " 'delight': 223,\n",
       " 'pittsburgh': 631,\n",
       " 'review': 716,\n",
       " 'sunday': 856,\n",
       " 'night': 560,\n",
       " 'drink': 257,\n",
       " 'bacon': 54,\n",
       " 'wrap': 989,\n",
       " 'fig': 318,\n",
       " 'simply': 783,\n",
       " 'got': 367,\n",
       " 'ent': 277,\n",
       " 'plat': 636,\n",
       " 'outstand': 591,\n",
       " 'hair': 380,\n",
       " 'tot': 907,\n",
       " 'whil': 970,\n",
       " 'yo': 996,\n",
       " 'talk': 866,\n",
       " 'phon': 623,\n",
       " 'instead': 435,\n",
       " 'person': 619,\n",
       " 'doe': 246,\n",
       " 'fac': 300,\n",
       " 'work': 984,\n",
       " 'glad': 361,\n",
       " 'suit': 854,\n",
       " 'anyth': 28,\n",
       " 'grab': 368,\n",
       " 'som': 801,\n",
       " 'lunch': 495,\n",
       " 'pet': 620,\n",
       " 'shrimp': 778,\n",
       " 'yum': 998,\n",
       " 'spicy': 820,\n",
       " 'particul': 606,\n",
       " 'way': 957,\n",
       " 'tun': 923,\n",
       " 'crab': 192,\n",
       " 'stay': 835,\n",
       " 'away': 51,\n",
       " 'strip': 847,\n",
       " 'stop': 842,\n",
       " 'near': 553,\n",
       " 'know': 453,\n",
       " 'peopl': 615,\n",
       " 'becaus': 67,\n",
       " 'typ': 927,\n",
       " 'eat': 267,\n",
       " 'hat': 388,\n",
       " 'went': 966,\n",
       " 'chant': 134,\n",
       " 'anoth': 25,\n",
       " 'plac': 634,\n",
       " 'wait': 946,\n",
       " 'step': 838,\n",
       " 'told': 901,\n",
       " 'ov': 592,\n",
       " 'spac': 814,\n",
       " 'bar': 59,\n",
       " 'din': 238,\n",
       " 'everyth': 286,\n",
       " 'beef': 70,\n",
       " 'bed': 69,\n",
       " 'sort': 809,\n",
       " 'sav': 750,\n",
       " 'dessert': 232,\n",
       " 'sound': 810,\n",
       " 'split': 822,\n",
       " 'fantast': 308,\n",
       " 'knowledg': 454,\n",
       " 'menu': 522,\n",
       " 'help': 398,\n",
       " 'decid': 218,\n",
       " 'ev': 284,\n",
       " 'ful': 348,\n",
       " 'non': 561,\n",
       " 'sometim': 804,\n",
       " 'busy': 113,\n",
       " 'say': 752,\n",
       " 'plan': 635,\n",
       " 'tip': 895,\n",
       " 'quit': 682,\n",
       " 'mem': 518,\n",
       " 'cat': 127,\n",
       " 'notch': 564,\n",
       " 'weird': 963,\n",
       " 'pool': 645,\n",
       " 'feel': 316,\n",
       " 'lik': 475,\n",
       " 'whol': 972,\n",
       " 'leav': 466,\n",
       " 'airport': 17,\n",
       " 'board': 85,\n",
       " 'pass': 608,\n",
       " 'suck': 851,\n",
       " 'high': 400,\n",
       " 'hel': 397,\n",
       " 'deal': 216,\n",
       " 'old': 576,\n",
       " 'disappoint': 242,\n",
       " 'eith': 270,\n",
       " 'limit': 476,\n",
       " 'best': 76,\n",
       " 'someon': 802,\n",
       " 'man': 503,\n",
       " 'beer': 71,\n",
       " 'cho': 145,\n",
       " 'plenty': 639,\n",
       " 'far': 309,\n",
       " 'rest': 712,\n",
       " 'sam': 742,\n",
       " 'dish': 244,\n",
       " 'cam': 119,\n",
       " 'sandwich': 744,\n",
       " 'yummy': 999,\n",
       " 'big': 78,\n",
       " 'town': 911,\n",
       " 'giv': 360,\n",
       " 'shot': 777,\n",
       " 'return': 715,\n",
       " 'sev': 771,\n",
       " 'ric': 718,\n",
       " 'els': 271,\n",
       " 'incred': 428,\n",
       " 'thank': 882,\n",
       " 'start': 833,\n",
       " 'miss': 532,\n",
       " 'chil': 142,\n",
       " 'salt': 741,\n",
       " 'tradit': 912,\n",
       " 'hang': 384,\n",
       " 'steak': 836,\n",
       " 'bad': 55,\n",
       " 'est': 282,\n",
       " 'friday': 344,\n",
       " 'pm': 641,\n",
       " 'overal': 593,\n",
       " 'pretty': 662,\n",
       " 'impress': 425,\n",
       " 'group': 374,\n",
       " 'larg': 461,\n",
       " 'whit': 971,\n",
       " 'pizz': 632,\n",
       " 'garl': 353,\n",
       " 'comp': 166,\n",
       " 'somewh': 805,\n",
       " 'bit': 81,\n",
       " 'receiv': 694,\n",
       " 'littl': 480,\n",
       " 'aft': 11,\n",
       " 'brunch': 105,\n",
       " 'morn': 543,\n",
       " 'weekend': 962,\n",
       " 'staff': 828,\n",
       " 'french': 340,\n",
       " 'toast': 897,\n",
       " 'suggest': 853,\n",
       " 'omelet': 577,\n",
       " 'saus': 749,\n",
       " 'coff': 158,\n",
       " 'buck': 106,\n",
       " 'worst': 987,\n",
       " 'son': 806,\n",
       " 'nev': 557,\n",
       " 'honest': 406,\n",
       " 'cal': 117,\n",
       " 'day': 215,\n",
       " 'dant': 210,\n",
       " 'tonight': 904,\n",
       " 'short': 776,\n",
       " 'exceiv': 288,\n",
       " 'piec': 629,\n",
       " 'hug': 416,\n",
       " 'head': 390,\n",
       " 'bet': 77,\n",
       " 'pay': 613,\n",
       " 'salon': 739,\n",
       " 'thes': 884,\n",
       " 'lack': 458,\n",
       " 'wast': 954,\n",
       " 'heart': 394,\n",
       " 'mov': 545,\n",
       " 'cool': 182,\n",
       " 'play': 637,\n",
       " 'host': 409,\n",
       " 'city': 150,\n",
       " 'regul': 699,\n",
       " 'chair': 132,\n",
       " 'watch': 956,\n",
       " 'sport': 824,\n",
       " 'window': 976,\n",
       " 'bunch': 110,\n",
       " 'card': 121,\n",
       " 'expend': 292,\n",
       " 'unless': 931,\n",
       " 'check': 138,\n",
       " 'chang': 133,\n",
       " 'week': 961,\n",
       " 'oft': 570,\n",
       " 'tak': 865,\n",
       " 'mat': 508,\n",
       " 'atmosph': 43,\n",
       " 'sens': 765,\n",
       " 'fun': 349,\n",
       " 'run': 731,\n",
       " 'touch': 908,\n",
       " 'fix': 325,\n",
       " 'mistak': 533,\n",
       " 'exact': 287,\n",
       " 'took': 905,\n",
       " 'explain': 294,\n",
       " 'wrong': 991,\n",
       " 'valu': 934,\n",
       " 'wor': 982,\n",
       " 'blu': 84,\n",
       " 'turn': 924,\n",
       " 'wat': 955,\n",
       " 'heat': 395,\n",
       " 'nee': 554,\n",
       " 'learn': 465,\n",
       " 'handl': 383,\n",
       " 'recommend': 695,\n",
       " 'lamb': 460,\n",
       " 'curry': 206,\n",
       " 'chick': 141,\n",
       " 'norm': 563,\n",
       " 'extr': 296,\n",
       " 'greasy': 370,\n",
       " 'heavy': 396,\n",
       " 'port': 649,\n",
       " 'col': 159,\n",
       " 'beat': 65,\n",
       " 'avail': 48,\n",
       " 'monday': 538,\n",
       " 'oth': 587,\n",
       " 'school': 753,\n",
       " 'slow': 794,\n",
       " 'bland': 83,\n",
       " 'suppos': 858,\n",
       " 'paid': 598,\n",
       " 'minut': 531,\n",
       " 'cold': 160,\n",
       " 'bef': 72,\n",
       " 'reserv': 710,\n",
       " 'memb': 519,\n",
       " 'pap': 603,\n",
       " 'sorry': 808,\n",
       " 'greet': 372,\n",
       " 'ey': 298,\n",
       " 'ear': 264,\n",
       " 'saf': 734,\n",
       " 'coupl': 187,\n",
       " 'machin': 497,\n",
       " 'job': 440,\n",
       " 'consid': 174,\n",
       " 'pict': 627,\n",
       " 'bring': 99,\n",
       " 'gav': 354,\n",
       " 'fre': 339,\n",
       " 'rid': 720,\n",
       " 'hotel': 412,\n",
       " 'noth': 565,\n",
       " 'ok': 573,\n",
       " 'diff': 235,\n",
       " 'hous': 414,\n",
       " 'party': 607,\n",
       " 'guy': 378,\n",
       " 'rock': 726,\n",
       " 'terr': 877,\n",
       " 'walk': 949,\n",
       " 'forgot': 336,\n",
       " 'anyon': 27,\n",
       " 'charg': 135,\n",
       " 'meal': 511,\n",
       " 'del': 221,\n",
       " 'prevy': 663,\n",
       " 'complain': 168,\n",
       " 'bas': 61,\n",
       " 'past': 609,\n",
       " 'second': 759,\n",
       " 'gril': 373,\n",
       " 'wond': 981,\n",
       " 'welcom': 965,\n",
       " 'salmon': 738,\n",
       " 'tart': 868,\n",
       " 'fresh': 342,\n",
       " 'wish': 979,\n",
       " 'fish': 322,\n",
       " 'black': 82,\n",
       " 'standard': 830,\n",
       " 'lemon': 469,\n",
       " 'ol': 575,\n",
       " 'oil': 572,\n",
       " 'cours': 189,\n",
       " 'satisfy': 746,\n",
       " 'end': 275,\n",
       " 'hap': 385,\n",
       " 'kitch': 451,\n",
       " 'box': 92,\n",
       " 'smal': 795,\n",
       " 'famy': 305,\n",
       " 'donut': 251,\n",
       " 'yelp': 995,\n",
       " 'discount': 243,\n",
       " 'vary': 935,\n",
       " 'appl': 36,\n",
       " 'sug': 852,\n",
       " 'cup': 203,\n",
       " 'road': 724,\n",
       " 'kind': 450,\n",
       " 'lat': 463,\n",
       " 'wear': 958,\n",
       " 'burg': 111,\n",
       " 'med': 514,\n",
       " 'rar': 686,\n",
       " 'bun': 109,\n",
       " 'complaint': 169,\n",
       " 'lettuc': 471,\n",
       " 'hard': 387,\n",
       " 'otherw': 588,\n",
       " 'tasty': 870,\n",
       " 'breakfast': 98,\n",
       " 'burrito': 112,\n",
       " 'wow': 988,\n",
       " 'trust': 920,\n",
       " 'wing': 977,\n",
       " 'afternoon': 12,\n",
       " 'clos': 154,\n",
       " 'cent': 129,\n",
       " 'outsid': 590,\n",
       " 'thought': 890,\n",
       " 'modern': 535,\n",
       " 'pot': 653,\n",
       " 'mou': 544,\n",
       " 'fan': 306,\n",
       " 'girlfriend': 359,\n",
       " 'intery': 436,\n",
       " 'posit': 650,\n",
       " 'sat': 745,\n",
       " 'environ': 279,\n",
       " 'red': 696,\n",
       " 'surpr': 860,\n",
       " 'felt': 317,\n",
       " 'sery': 769,\n",
       " 'kept': 446,\n",
       " 'attend': 44,\n",
       " 'ask': 41,\n",
       " 'excit': 290,\n",
       " 'siz': 789,\n",
       " 'appet': 35,\n",
       " 'chip': 144,\n",
       " 'vega': 937,\n",
       " 'produc': 671,\n",
       " 'healthy': 391,\n",
       " 'cur': 205,\n",
       " 'caf': 115,\n",
       " 'scottsd': 754,\n",
       " 'left': 467,\n",
       " 'spok': 823,\n",
       " 'gift': 357,\n",
       " 'lit': 479,\n",
       " 'extrem': 297,\n",
       " 'accommod': 4,\n",
       " 'veget': 938,\n",
       " 'veg': 936,\n",
       " 'car': 120,\n",
       " 'bel': 74,\n",
       " 'ingredy': 431,\n",
       " 'spic': 819,\n",
       " 'kick': 448,\n",
       " 'sit': 787,\n",
       " 'mediocr': 515,\n",
       " 'mix': 534,\n",
       " 'cream': 196,\n",
       " 'slight': 793,\n",
       " 'level': 472,\n",
       " 'wif': 973,\n",
       " 'st': 827,\n",
       " 'bottl': 89,\n",
       " 'win': 975,\n",
       " 'chop': 148,\n",
       " 'roast': 725,\n",
       " 'mash': 506,\n",
       " 'prob': 668,\n",
       " 'tir': 896,\n",
       " 'buy': 114,\n",
       " 'quot': 683,\n",
       " 'stat': 834,\n",
       " 'ready': 690,\n",
       " 'east': 265,\n",
       " 'import': 424,\n",
       " 'hop': 407,\n",
       " 'replac': 708,\n",
       " 'becom': 68,\n",
       " 'seat': 757,\n",
       " 'immedy': 423,\n",
       " 'gem': 355,\n",
       " 'fact': 302,\n",
       " 'sec': 758,\n",
       " 'comfort': 164,\n",
       " 'rud': 730,\n",
       " 'half': 381,\n",
       " 'employ': 274,\n",
       " 'speak': 815,\n",
       " 'wher': 969,\n",
       " 'dirty': 241,\n",
       " 'mark': 504,\n",
       " 'correct': 184,\n",
       " 'girl': 358,\n",
       " 'count': 186,\n",
       " 'hand': 382,\n",
       " 'taco': 864,\n",
       " 'neighb': 556,\n",
       " 'av': 47,\n",
       " 'la': 457,\n",
       " 'fit': 323,\n",
       " 'mon': 537,\n",
       " 'espec': 281,\n",
       " 'market': 505,\n",
       " 'downtown': 254,\n",
       " 'starbuck': 832,\n",
       " 'doing': 248,\n",
       " 'amby': 23,\n",
       " 'opin': 581,\n",
       " 'describ': 227,\n",
       " 'cas': 123,\n",
       " 'sampl': 743,\n",
       " 'pack': 596,\n",
       " 'auth': 46,\n",
       " 'joint': 441,\n",
       " 'tre': 915,\n",
       " 'rememb': 702,\n",
       " 'juic': 443,\n",
       " 'cak': 116,\n",
       " 'upd': 932,\n",
       " 'travel': 914,\n",
       " 'sign': 780,\n",
       " 'avoid': 49,\n",
       " 'mushroom': 549,\n",
       " 'pick': 625,\n",
       " 'bal': 58,\n",
       " 'prop': 674,\n",
       " 'creamy': 197,\n",
       " 'milk': 528,\n",
       " 'smel': 796,\n",
       " 'thorough': 888,\n",
       " 'dark': 211,\n",
       " 'websit': 959,\n",
       " 'oh': 571,\n",
       " 'goe': 363,\n",
       " 'prompt': 673,\n",
       " 'rel': 700,\n",
       " 'org': 585,\n",
       " 'pizza': 633,\n",
       " 'saw': 751,\n",
       " 'multipl': 547,\n",
       " 'confus': 173,\n",
       " 'direct': 240,\n",
       " 'opt': 582,\n",
       " 'valley': 933,\n",
       " 'conveny': 178,\n",
       " 'wash': 953,\n",
       " 'imagin': 422,\n",
       " 'difficult': 236,\n",
       " 'paty': 612,\n",
       " 'book': 88,\n",
       " 'pref': 658,\n",
       " 'sect': 760,\n",
       " 'movy': 546,\n",
       " 'tv': 925,\n",
       " 'post': 652,\n",
       " 'sel': 762,\n",
       " 'onlin': 578,\n",
       " 'gen': 356,\n",
       " 'lucky': 494,\n",
       " 'kid': 449,\n",
       " 'folk': 331,\n",
       " 'mil': 527,\n",
       " 'doll': 249,\n",
       " 'poss': 651,\n",
       " 'cash': 124,\n",
       " 'leg': 468,\n",
       " 'lady': 459,\n",
       " 'photo': 624,\n",
       " 'cost': 185,\n",
       " 'casino': 126,\n",
       " 'tha': 881,\n",
       " 'ic': 419,\n",
       " 'die': 234,\n",
       " 'las': 462,\n",
       " 'test': 878,\n",
       " 'prep': 659,\n",
       " 'mal': 502,\n",
       " 'access': 3,\n",
       " 'smil': 797,\n",
       " 'fast': 310,\n",
       " 'efficy': 268,\n",
       " 'star': 831,\n",
       " 'issu': 437,\n",
       " 'pop': 647,\n",
       " 'mass': 507,\n",
       " 'fiv': 324,\n",
       " 'list': 478,\n",
       " 'relax': 701,\n",
       " 'brown': 104,\n",
       " 'nat': 552,\n",
       " 'tel': 874,\n",
       " 'melt': 517,\n",
       " 'tea': 872,\n",
       " 'rec': 693,\n",
       " 'rat': 687,\n",
       " 'provid': 675,\n",
       " 'bil': 79,\n",
       " 'solid': 800,\n",
       " 'groupon': 375,\n",
       " 'coupon': 188,\n",
       " 'daught': 214,\n",
       " 'crazy': 194,\n",
       " 'hear': 392,\n",
       " 'stuff': 849,\n",
       " 'form': 337,\n",
       " 'design': 229,\n",
       " 'ins': 432,\n",
       " 'sals': 740,\n",
       " 'clear': 153,\n",
       " 'mayb': 510,\n",
       " 'press': 661,\n",
       " 'tiny': 894,\n",
       " 'fav': 312,\n",
       " 'soup': 811,\n",
       " 'bought': 90,\n",
       " 'spring': 826,\n",
       " 'fil': 319,\n",
       " 'complet': 170,\n",
       " 'mean': 512,\n",
       " 'cheap': 137,\n",
       " 'nam': 551,\n",
       " 'chain': 131,\n",
       " 'alon': 20,\n",
       " 'waitress': 947,\n",
       " 'train': 913,\n",
       " 'bowl': 91,\n",
       " 'lin': 477,\n",
       " 'smok': 798,\n",
       " 'door': 252,\n",
       " 'pres': 660,\n",
       " 'sist': 786,\n",
       " 'birthday': 80,\n",
       " 'seen': 761,\n",
       " 'anywh': 29,\n",
       " 'allow': 19,\n",
       " 'favourit': 314,\n",
       " 'street': 846,\n",
       " 'men': 520,\n",
       " 'chocol': 146,\n",
       " 'fut': 351,\n",
       " 'waffl': 945,\n",
       " 'pancak': 602,\n",
       " 'purchas': 677,\n",
       " 'low': 493,\n",
       " 'crowd': 201,\n",
       " 'mus': 548,\n",
       " 'glass': 362,\n",
       " 'perhap': 618,\n",
       " 'attitud': 45,\n",
       " 'mess': 523,\n",
       " 'understand': 929,\n",
       " 'rent': 705,\n",
       " 'knew': 452,\n",
       " 'poor': 646,\n",
       " 'carry': 122,\n",
       " 'ahead': 15,\n",
       " 'whatev': 968,\n",
       " 'ram': 684,\n",
       " 'guess': 376,\n",
       " 'world': 985,\n",
       " 'japanes': 439,\n",
       " 'known': 455,\n",
       " 'flavo': 328,\n",
       " 'saturday': 747,\n",
       " 'trip': 917,\n",
       " 'view': 943,\n",
       " 'okay': 574,\n",
       " 'certain': 130,\n",
       " 'occas': 568,\n",
       " 'sum': 855,\n",
       " 'repair': 707,\n",
       " 'crispy': 200,\n",
       " 'juicy': 444,\n",
       " 'singl': 784,\n",
       " 'tap': 867,\n",
       " 'annoy': 24,\n",
       " 'ticket': 891,\n",
       " 'quiet': 681,\n",
       " 'club': 156,\n",
       " 'phoenix': 622,\n",
       " 'set': 770,\n",
       " 'flight': 329,\n",
       " 'soon': 807,\n",
       " 'simil': 781,\n",
       " 'forward': 338,\n",
       " 'gym': 379,\n",
       " 'dip': 239,\n",
       " 'combin': 162,\n",
       " 'ago': 14,\n",
       " 'horr': 408,\n",
       " 'sep': 767,\n",
       " 'filet': 320,\n",
       " 'boyfriend': 94,\n",
       " 'od': 569,\n",
       " 'seafood': 755,\n",
       " 'pricey': 666,\n",
       " 'hit': 401,\n",
       " 'room': 728,\n",
       " 'class': 151,\n",
       " 'word': 983,\n",
       " 'adv': 9,\n",
       " 'gam': 352,\n",
       " 'delivery': 224,\n",
       " 'ind': 429,\n",
       " 'contact': 176,\n",
       " 'read': 689,\n",
       " 'writ': 990,\n",
       " 'company': 167,\n",
       " 'entir': 278,\n",
       " 'bathroom': 62,\n",
       " 'cov': 191,\n",
       " 'concern': 172,\n",
       " 'sent': 766,\n",
       " 'text': 879,\n",
       " 'brought': 103,\n",
       " 'paint': 600,\n",
       " 'respons': 711,\n",
       " 'twic': 926,\n",
       " 'ign': 421,\n",
       " 'instal': 434,\n",
       " 'til': 892,\n",
       " 'prim': 667,\n",
       " 'continu': 177,\n",
       " 'obvy': 567,\n",
       " 'forget': 335,\n",
       " 'admit': 8,\n",
       " 'believ': 75,\n",
       " 'driv': 258,\n",
       " 'cupcak': 204,\n",
       " 'key': 447,\n",
       " 'reg': 698,\n",
       " 'desk': 230,\n",
       " 'min': 529,\n",
       " 'guest': 377,\n",
       " 'credit': 198,\n",
       " 'remind': 703,\n",
       " 'venu': 941,\n",
       " 'loung': 491,\n",
       " 'outdo': 589,\n",
       " 'patio': 610,\n",
       " 'cre': 195,\n",
       " 'tax': 871,\n",
       " 'dat': 213,\n",
       " 'homemad': 405,\n",
       " 'desert': 228,\n",
       " 'rep': 706,\n",
       " 'lol': 484,\n",
       " 'frequ': 341,\n",
       " 'met': 524,\n",
       " 'overpr': 594,\n",
       " 'stick': 839,\n",
       " 'ap': 30,\n",
       " 'bro': 101,\n",
       " 'hol': 402,\n",
       " 'art': 40,\n",
       " 'pract': 657,\n",
       " 'dr': 255,\n",
       " 'numb': 566,\n",
       " 'doct': 245,\n",
       " 'maj': 500,\n",
       " 'dent': 225,\n",
       " 'main': 499,\n",
       " 'fee': 315,\n",
       " 'jok': 442,\n",
       " 'slid': 792,\n",
       " 'air': 16,\n",
       " 'meet': 516,\n",
       " 'rush': 732,\n",
       " 'ment': 521,\n",
       " 'begin': 73,\n",
       " 'pep': 616,\n",
       " 'slic': 791,\n",
       " 'togeth': 900,\n",
       " 'dry': 260,\n",
       " 'tomato': 902,\n",
       " 'loud': 490,\n",
       " 'sad': 733,\n",
       " 'wal': 948,\n",
       " 'improv': 426,\n",
       " 'buffet': 107,\n",
       " 'mom': 536,\n",
       " 'los': 487,\n",
       " 'chos': 149,\n",
       " 'boy': 93,\n",
       " 'process': 670,\n",
       " 'flat': 326,\n",
       " 'inform': 430,\n",
       " 'tow': 910,\n",
       " 'nail': 550,\n",
       " 'charlot': 136,\n",
       " 'ide': 420,\n",
       " 'fruit': 347,\n",
       " 'skin': 790,\n",
       " 'spinach': 821,\n",
       " 'crust': 202,\n",
       " 'soft': 799,\n",
       " 'duck': 261,\n",
       " 'situ': 788,\n",
       " 'build': 108,\n",
       " 'dog': 247,\n",
       " 'celebr': 128,\n",
       " 'ag': 13,\n",
       " 'forev': 334,\n",
       " 'hostess': 410,\n",
       " 'break': 97,\n",
       " 'court': 190,\n",
       " 'ped': 614,\n",
       " 'cooky': 181,\n",
       " 'extend': 295,\n",
       " 'email': 273,\n",
       " 'elsewh': 272,\n",
       " 'doubl': 253,\n",
       " 'gon': 365,\n",
       " 'dim': 237,\n",
       " 'chines': 143,\n",
       " 'crep': 199,\n",
       " 'veggy': 939,\n",
       " 'choos': 147,\n",
       " 'straight': 845,\n",
       " 'story': 844,\n",
       " 'tour': 909,\n",
       " 'par': 604,\n",
       " 'wom': 980,\n",
       " 'brand': 95,\n",
       " 'war': 951,\n",
       " 'rich': 719,\n",
       " 'follow': 332,\n",
       " 'lobst': 482,\n",
       " 'calamar': 118,\n",
       " 'yeah': 993,\n",
       " 'tofu': 899,\n",
       " 'refil': 697,\n",
       " 'remov': 704,\n",
       " 'temp': 875,\n",
       " 'wors': 986,\n",
       " 'cloth': 155,\n",
       " 'dam': 209,\n",
       " 'abov': 1,\n",
       " 'sour': 812,\n",
       " 'pain': 599,\n",
       " 'cashy': 125,\n",
       " 'ridic': 721,\n",
       " 'quest': 679,\n",
       " 'young': 997,\n",
       " 'aw': 50,\n",
       " 'oyst': 595,\n",
       " 'neg': 555,\n",
       " 'chef': 140,\n",
       " 'polit': 644,\n",
       " 'ring': 723,\n",
       " 'fal': 304,\n",
       " 'das': 212,\n",
       " 'ein': 269,\n",
       " 'der': 226,\n",
       " 'vib': 942,\n",
       " 'kor': 456,\n",
       " 'consist': 175,\n",
       " 'ye': 992,\n",
       " 'hungry': 417,\n",
       " 'pour': 655,\n",
       " 'steam': 837,\n",
       " 'toronto': 906,\n",
       " 'crav': 193,\n",
       " 'fab': 299,\n",
       " 'apart': 31,\n",
       " 'answ': 26,\n",
       " 'facil': 301,\n",
       " 'deep': 219,\n",
       " 'shak': 772,\n",
       " 'pri': 664,\n",
       " 'ass': 42,\n",
       " 'pad': 597,\n",
       " 'pol': 643,\n",
       " 'team': 873,\n",
       " 'funny': 350,\n",
       " 'wir': 978,\n",
       " 'commun': 165,\n",
       " 'middl': 526,\n",
       " 'noodl': 562,\n",
       " 'tuesday': 922,\n",
       " 'app': 33,\n",
       " 'vehic': 940,\n",
       " 'dumpl': 262,\n",
       " 'orang': 583,\n",
       " 'body': 86,\n",
       " 'send': 764,\n",
       " 'appear': 34,\n",
       " 'poutin': 656,\n",
       " 'bartend': 60,\n",
       " 'result': 714,\n",
       " 'request': 709,\n",
       " 'brok': 102,\n",
       " 'pickl': 626,\n",
       " 'strong': 848,\n",
       " 'fat': 311,\n",
       " 'lost': 488,\n",
       " 'match': 509,\n",
       " 'spa': 813,\n",
       " 'combo': 163,\n",
       " 'wed': 960,\n",
       " 'froz': 346,\n",
       " 'le': 464,\n",
       " 'et': 283,\n",
       " 'pho': 621,\n",
       " 'baby': 53,\n",
       " 'mont': 540,\n",
       " 'patron': 611,\n",
       " 'es': 280,\n",
       " 'pit': 630,\n",
       " 'brisket': 100,\n",
       " 'truck': 919}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour afficher le vocabulaire gardé par TFIDF\n",
    "TFIDF.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## RF #############\n",
      "MAE = 0.745, Accuracy =0.451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 544,  848,  483,  241,   28],\n",
       "       [  75,  393,  530,  341,   39],\n",
       "       [  25,  184,  705,  885,   88],\n",
       "       [   5,   98,  613, 2258,  687],\n",
       "       [   7,  109,  478, 2812, 3157]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## KNN #############\n",
      "MAE = 0.909, Accuracy =0.361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 323,  663,  648,  439,   71],\n",
       "       [  61,  245,  519,  477,   76],\n",
       "       [  21,  169,  614,  946,  137],\n",
       "       [  13,  128,  811, 2061,  648],\n",
       "       [  20,  140,  919, 3079, 2405]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## MLP #############\n",
      "MAE = 0.745, Accuracy =0.513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1353,  366,  186,  142,   97],\n",
       "       [ 419,  329,  295,  214,  121],\n",
       "       [ 201,  325,  486,  547,  328],\n",
       "       [ 141,  250,  560, 1262, 1448],\n",
       "       [ 132,  191,  409, 1249, 4582]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_models (corpus_train_tfidf.toarray(),y_train,corpus_test_tfidf.toarray(),y_test,algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utlisant TF-IDF on remarque que le KNN a les moins bons résultats cela est du a difficulté de calculer les distance dans un espace de trés grande dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application de la SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une SVD qui garde 100 composantes les plus importantes \n",
    "SVD=TruncatedSVD(n_components=100)\n",
    "# Application de la SVD sur sur le corpus train et test obtenu précedemment avec TF-IDF\n",
    "SVD.fit(corpus_train_tfidf)\n",
    "corpus_train_SVD=SVD.transform(corpus_train_tfidf)\n",
    "corpus_test_SVD=SVD.transform(corpus_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## RF #############\n",
      "MAE = 0.789, Accuracy =0.417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 290,  933,  666,  238,   17],\n",
       "       [  31,  355,  652,  319,   21],\n",
       "       [   5,  163,  872,  787,   60],\n",
       "       [   2,   83,  901, 2152,  523],\n",
       "       [   5,  108,  648, 2949, 2853]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## KNN #############\n",
      "MAE = 0.872, Accuracy =0.386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 533,  735,  540,  301,   35],\n",
       "       [  99,  377,  513,  344,   45],\n",
       "       [  35,  223,  765,  764,  100],\n",
       "       [  25,  175, 1017, 1863,  581],\n",
       "       [  38,  224, 1022, 2775, 2504]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## MLP #############\n",
      "MAE = 0.668, Accuracy =0.509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 839,  811,  313,  157,   24],\n",
       "       [ 175,  467,  474,  235,   27],\n",
       "       [  51,  301,  716,  736,   83],\n",
       "       [  14,  126,  647, 2071,  803],\n",
       "       [  16,  132,  393, 2163, 3859]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_models (corpus_train_SVD,y_train,corpus_test_SVD,y_test,algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'avec la SVD l'algorithme de KNN augmente un peux ses performances cela est du a la réduction de la dimension par contre l'algorithme random forest perd en performance car a cause de la SVD il y a moins d'héterogéniété dans les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation des embedding word to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens=corpus.apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [board, my, engl, mastiff, her, ov, new, year,...\n",
       "1        [anoth, cas, of, the, emp, new, cloth, someon,...\n",
       "2        [cam, on, valentin, day, night, hav, prebought...\n",
       "3        [nd, tim, eat, her, todayst, tim, was, gre, bu...\n",
       "4        [allegy, is, a, disast, their, far, ar, cheap,...\n",
       "                               ...                        \n",
       "47366    [thi, is, our, favorit, coff, plac, in, mont, ...\n",
       "47367    [had, to, visit, the, carlo, bakery, and, went...\n",
       "47368    [som, of, the, best, tom, yum, we, hav, ev, ha...\n",
       "47369    [thi, is, the, best, groom, in, the, valley, s...\n",
       "47370    [i, agr, with, the, oth, review, thi, is, a, g...\n",
       "Length: 47371, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores=multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un modele W2V de taille 100 entrainné sur nos données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size=100\n",
    "model=gensim.models.Word2Vec(corpus_tokens,vector_size=model_size,sg=0,window=5,min_count=2,workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainnement du modele "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    model.train(corpus_tokens,total_examples=len(corpus),epochs=1)\n",
    "    print(i, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/Word2vec_entraine.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement du modele\n",
    "model=gensim.models.Word2Vec.load('models/Word2vec_entraine.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22629"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour generer l'embedding d'un texte en utilisant un modele W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get word2vec for each sentences by using average word embeddings\n",
    "def word2vec_generator(texts,model,vector_size):\n",
    "    dict_word2vec = {}\n",
    "    for index, word_list in enumerate(texts):\n",
    "        arr = np.array([0.0 for i in range(0, vector_size)])\n",
    "        nb_word=0\n",
    "        for word in word_list:\n",
    "            try:\n",
    "                arr += model[word]\n",
    "                nb_word=nb_word+1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if(len(word_list) == 0):\n",
    "            dict_word2vec[index] = arr\n",
    "        else:\n",
    "            dict_word2vec[index] = arr / nb_word\n",
    "    df_word2vec = pd.DataFrame(dict_word2vec).T\n",
    "    return df_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['board',\n",
       " 'my',\n",
       " 'engl',\n",
       " 'mastiff',\n",
       " 'her',\n",
       " 'ov',\n",
       " 'new',\n",
       " 'year',\n",
       " 'they',\n",
       " 'ar',\n",
       " 'clos',\n",
       " 'on',\n",
       " 'sunday',\n",
       " 'so',\n",
       " 'ev',\n",
       " 'though',\n",
       " 'we',\n",
       " 'got',\n",
       " 'hom',\n",
       " 'on',\n",
       " 'sunday',\n",
       " 'we',\n",
       " 'had',\n",
       " 'to',\n",
       " 'leav',\n",
       " 'him',\n",
       " 'her',\n",
       " 'anoth',\n",
       " 'day',\n",
       " 'and',\n",
       " 'pay',\n",
       " 'for',\n",
       " 'anoth',\n",
       " 'day',\n",
       " 'becaus',\n",
       " 'we',\n",
       " 'could',\n",
       " 'not',\n",
       " 'pick',\n",
       " 'him',\n",
       " 'up',\n",
       " 'until',\n",
       " 'monday',\n",
       " 'morn',\n",
       " 'he',\n",
       " 'got',\n",
       " 'a',\n",
       " 'comply',\n",
       " 'bath',\n",
       " 'with',\n",
       " 'his',\n",
       " 'board',\n",
       " 'when',\n",
       " 'i',\n",
       " 'did',\n",
       " 'pick',\n",
       " 'him',\n",
       " 'up',\n",
       " 'ont',\n",
       " 'i',\n",
       " 'got',\n",
       " 'him',\n",
       " 'in',\n",
       " 'the',\n",
       " 'car',\n",
       " 'i',\n",
       " 'not',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pung',\n",
       " 'musky',\n",
       " 'od',\n",
       " 'could',\n",
       " 'not',\n",
       " 'fig',\n",
       " 'out',\n",
       " 'what',\n",
       " 'it',\n",
       " 'was',\n",
       " 'until',\n",
       " 'i',\n",
       " 'got',\n",
       " 'him',\n",
       " 'hom',\n",
       " 'the',\n",
       " 'smel',\n",
       " 'was',\n",
       " 'so',\n",
       " 'bad',\n",
       " 'i',\n",
       " 'had',\n",
       " 'to',\n",
       " 'leav',\n",
       " 'the',\n",
       " 'car',\n",
       " 'window',\n",
       " 'op',\n",
       " 'to',\n",
       " 'air',\n",
       " 'it',\n",
       " 'out',\n",
       " 'he',\n",
       " 'had',\n",
       " 'a',\n",
       " 'green',\n",
       " 'mat',\n",
       " 'on',\n",
       " 'the',\n",
       " 'undersid',\n",
       " 'of',\n",
       " 'his',\n",
       " 'tail',\n",
       " 'and',\n",
       " 'around',\n",
       " 'his',\n",
       " 'but',\n",
       " 'i',\n",
       " 'tri',\n",
       " 'to',\n",
       " 'cle',\n",
       " 'it',\n",
       " 'off',\n",
       " 'let',\n",
       " 'him',\n",
       " 'outsid',\n",
       " 'and',\n",
       " 'he',\n",
       " 'had',\n",
       " 'watery',\n",
       " 'diarrhe',\n",
       " 'i',\n",
       " 'cal',\n",
       " 'them',\n",
       " 'and',\n",
       " 'ask',\n",
       " 'why',\n",
       " 'the',\n",
       " 'stinky',\n",
       " 'green',\n",
       " 'stuff',\n",
       " 'was',\n",
       " 'not',\n",
       " 'not',\n",
       " 'by',\n",
       " 'their',\n",
       " 'groom',\n",
       " 'they',\n",
       " 'said',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'it',\n",
       " 'and',\n",
       " 'fig',\n",
       " 'he',\n",
       " 'had',\n",
       " 'rol',\n",
       " 'in',\n",
       " 'someth',\n",
       " 'they',\n",
       " 'did',\n",
       " 'not',\n",
       " 'cle',\n",
       " 'it',\n",
       " 'al',\n",
       " 'off',\n",
       " 'and',\n",
       " 'sent',\n",
       " 'him',\n",
       " 'hom',\n",
       " 'that',\n",
       " 'way',\n",
       " 'it',\n",
       " 'was',\n",
       " 'an',\n",
       " 'gland',\n",
       " 'secret',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'think',\n",
       " 'why',\n",
       " 'a',\n",
       " 'groom',\n",
       " 'work',\n",
       " 'at',\n",
       " 'a',\n",
       " 'veterin',\n",
       " 'hospit',\n",
       " 'would',\n",
       " 'not',\n",
       " 'recogn',\n",
       " 'that',\n",
       " 'and',\n",
       " 'go',\n",
       " 'grab',\n",
       " 'a',\n",
       " 'tech',\n",
       " 'somewh',\n",
       " 'they',\n",
       " 'ar',\n",
       " 'al',\n",
       " 'ov',\n",
       " 'the',\n",
       " 'plac',\n",
       " 'to',\n",
       " 'hav',\n",
       " 'them',\n",
       " 'tak',\n",
       " 'a',\n",
       " 'look',\n",
       " 'i',\n",
       " 'had',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'my',\n",
       " 'poor',\n",
       " 'dog',\n",
       " 'al',\n",
       " 'the',\n",
       " 'way',\n",
       " 'back',\n",
       " 'ov',\n",
       " 'ther',\n",
       " 'mak',\n",
       " 'me',\n",
       " 'lat',\n",
       " 'for',\n",
       " 'work',\n",
       " 'for',\n",
       " 'someth',\n",
       " 'they',\n",
       " 'nev',\n",
       " 'should',\n",
       " 'hav',\n",
       " 'sent',\n",
       " 'him',\n",
       " 'hom',\n",
       " 'with',\n",
       " 'they',\n",
       " 'did',\n",
       " 'express',\n",
       " 'his',\n",
       " 'an',\n",
       " 'gland',\n",
       " 'but',\n",
       " 'brought',\n",
       " 'him',\n",
       " 'out',\n",
       " 'to',\n",
       " 'me',\n",
       " 'stil',\n",
       " 'stinky',\n",
       " 'and',\n",
       " 'leav',\n",
       " 'oi',\n",
       " 'green',\n",
       " 'stuff',\n",
       " 'wher',\n",
       " 'he',\n",
       " 'was',\n",
       " 'sit',\n",
       " 'in',\n",
       " 'their',\n",
       " 'lobby',\n",
       " 'i',\n",
       " 'ask',\n",
       " 'the',\n",
       " 'gal',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'count',\n",
       " 'what',\n",
       " 'is',\n",
       " 'thi',\n",
       " 'she',\n",
       " 'tri',\n",
       " 'to',\n",
       " 'tel',\n",
       " 'me',\n",
       " 'it',\n",
       " 'was',\n",
       " 'soap',\n",
       " 'i',\n",
       " 'ask',\n",
       " 'for',\n",
       " 'a',\n",
       " 'tissu',\n",
       " 'and',\n",
       " 'wip',\n",
       " 'it',\n",
       " 'up',\n",
       " 'show',\n",
       " 'it',\n",
       " 'to',\n",
       " 'her',\n",
       " 'and',\n",
       " 'ask',\n",
       " 'doe',\n",
       " 'thi',\n",
       " 'look',\n",
       " 'lik',\n",
       " 'soap',\n",
       " 'would',\n",
       " 'you',\n",
       " 'lik',\n",
       " 'to',\n",
       " 'tel',\n",
       " 'me',\n",
       " 'if',\n",
       " 'you',\n",
       " 'think',\n",
       " 'it',\n",
       " 'smel',\n",
       " 'lik',\n",
       " 'soap',\n",
       " 'they',\n",
       " 'again',\n",
       " 'took',\n",
       " 'him',\n",
       " 'back',\n",
       " 'and',\n",
       " 'cle',\n",
       " 'him',\n",
       " 'off',\n",
       " 'then',\n",
       " 'prescrib',\n",
       " 'antidiarrh',\n",
       " 'med',\n",
       " 'as',\n",
       " 'they',\n",
       " 'not',\n",
       " 'he',\n",
       " 'had',\n",
       " 'bad',\n",
       " 'diarrhe',\n",
       " 'whil',\n",
       " 'he',\n",
       " 'was',\n",
       " 'back',\n",
       " 'ther',\n",
       " 'i',\n",
       " 'wil',\n",
       " 'nev',\n",
       " 'board',\n",
       " 'my',\n",
       " 'dog',\n",
       " 'her',\n",
       " 'again']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train_tokens=corpus_train.apply(lambda x: x.split(\" \"))\n",
    "corpus_test_tokens=corpus_test.apply(lambda x: x.split(\" \"))\n",
    "corpus_train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size=model.vector_size\n",
    "# embedding des ensembles train et test avec le modele W2V \n",
    "corpus_train_wv_entraine=word2vec_generator(corpus_train_tokens,model.wv,vector_size)\n",
    "corpus_test_wv_entraine=word2vec_generator(corpus_test_tokens,model.wv,vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.343032\n",
       "1    -0.660038\n",
       "2     0.419913\n",
       "3     0.053538\n",
       "4    -0.233734\n",
       "        ...   \n",
       "95    0.419921\n",
       "96   -0.524470\n",
       "97   -0.120332\n",
       "98    0.087769\n",
       "99    0.692763\n",
       "Name: 0, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train_wv_entraine.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## RF #############\n",
      "MAE = 0.876, Accuracy =0.353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 114,  729,  962,  328,   11],\n",
       "       [  20,  237,  740,  371,   10],\n",
       "       [   1,  141,  879,  850,   16],\n",
       "       [   2,   74,  836, 2415,  334],\n",
       "       [   3,   56,  847, 3779, 1878]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## KNN #############\n",
      "MAE = 0.843, Accuracy =0.411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 438,  649,  640,  358,   59],\n",
       "       [  90,  258,  508,  461,   61],\n",
       "       [  41,  157,  591,  955,  143],\n",
       "       [  16,  120,  614, 2150,  761],\n",
       "       [  23,  161,  672, 2724, 2983]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## MLP #############\n",
      "MAE = 0.674, Accuracy =0.514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 695,  844,  419,  152,   34],\n",
       "       [ 121,  476,  519,  236,   26],\n",
       "       [  43,  276,  717,  751,  100],\n",
       "       [  17,  155,  642, 1908,  939],\n",
       "       [  14,  104,  423, 1785, 4237]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Execution des algorithmes de prédiction\n",
    "run_models (corpus_train_wv_entraine,y_train,corpus_test_wv_entraine,y_test,algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultat obtenu sont legerements meilleurs qu'avec les autres méthodes et ils peuvent etre amélioré en augmentant la taille de l'embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation du modele W2V de google de taille 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv_google=gensim.models.KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin.gz'\n",
    "                                                                , binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wv_google.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_train_tokens[0]\n",
    "vector_size=model_wv_google.vector_size\n",
    "corpus_train_wv_google=word2vec_generator(corpus_train_tokens,model_wv_google,vector_size)\n",
    "corpus_test_wv_google=word2vec_generator(corpus_test_tokens,model_wv_google,vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    }
   ],
   "source": [
    "nan_locations = corpus_train_wv_google.isnull().sum().sum()\n",
    "print(nan_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## le Word2Vec de google génere des valeurs null dans les embedding ce qui fait cracher les algo de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_train_wv_google\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcorpus_test_wv_google\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43malgos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m, in \u001b[0;36mrun_models\u001b[1;34m(X_train, Y_train, X_test, Y_test, algos)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m algo_name \u001b[38;5;129;01min\u001b[39;00m algos:\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39malgos[algo_name]\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     prediction\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      7\u001b[0m     prediction[prediction\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "run_models(corpus_train_wv_google,y_train,corpus_test_wv_google,y_test,algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nic', 'shop', 'easy', 'to', 'get', 'an', 'appoint', 'ladygirl', 'was', 'profess', 'friend', 'and', 'act', 'had', 'a', 'gre', 'convers', 'dur', 'the', 'cut', 'styl', 'was', 'perfect', 'and', 'serv', 'was', 'excel']\n",
      "[85, 244, 357, 4, 40, 61, 489, 5, 452, 65, 1, 218, 23, 3, 38, 910, 364, 0, 282, 423, 5, 202, 1, 28, 5, 249]\n"
     ]
    }
   ],
   "source": [
    "def to_sequence(index, text):\n",
    "    indexes = [index[word] for word in text if word in index]\n",
    "    return indexes\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(model.wv.index_to_key)}\n",
    "X_train_sequences = [to_sequence(word2idx, x) for x in corpus_train_tokens]\n",
    "X_test_sequences = [to_sequence(word2idx, x) for x in corpus_test_tokens]\n",
    "\n",
    "print(corpus_train_tokens.values[0])\n",
    "print(X_train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22629 22629 22629 22629 22629 22629 22629 22629 22629 22629 22629 22629\n",
      " 22629 22629 22629 22629 22629 22629 22629 22629 22629 22629 22629 22629\n",
      "    85   244   357     4    40    61   489     5   452    65     1   218\n",
      "    23     3    38   910   364     0   282   423     5   202     1    28\n",
      "     5   249]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGHT=50\n",
    "N_FEATURES = len(model.wv.index_to_key)\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
    "print(X_train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22630, 100)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDINGS_LEN = model.wv.vector_size\n",
    "embeddings_index = np.zeros((len(model.wv.index_to_key)+1, EMBEDDINGS_LEN))\n",
    "embeddings_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDINGS_LEN= 100\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_LEN = model.wv.vector_size\n",
    "embeddings_index = np.zeros((len(model.wv.index_to_key)+1, EMBEDDINGS_LEN))\n",
    "for word, idx in word2idx.items():\n",
    "    try:\n",
    "        embedding = model.wv[word]\n",
    "        embeddings_index[idx] = embedding\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         2263000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 300)         481200    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2904701 (11.08 MB)\n",
      "Trainable params: 641701 (2.45 MB)\n",
      "Non-trainable params: 2263000 (8.63 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense,LSTM\n",
    "\n",
    "# declaration de l'architecture du modele\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(len(model.wv.key_to_index)+1,\n",
    "                    EMBEDDINGS_LEN,  # Embedding size\n",
    "                    weights=[embeddings_index],\n",
    "                    trainable=False))\n",
    "\n",
    "#model_Bilstm.add(Embedding(30000,\n",
    "#                    300,  # Embedding size\n",
    "#                    input_length=50,\n",
    "#                    trainable=True))\n",
    "\n",
    "model_lstm.add(LSTM(300, dropout=0.2,return_sequences=True))\n",
    "model_lstm.add(LSTM(100, dropout=0.2))\n",
    "\n",
    "#model_lstm.add(Bidirectional(LSTM(128,recurrent_dropout=0.2, return_sequences=True),merge_mode='concat'))\n",
    "#model_lstm.add(Bidirectional(LSTM(128,recurrent_dropout=0.2, return_sequences=False),merge_mode='concat'))\n",
    "\n",
    "\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "model_lstm.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\etulyon1\\anaconda3\\envs\\gan\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "224/224 [==============================] - 67s 279ms/step - loss: 0.9108 - mean_absolute_error: 0.9108 - val_loss: 0.7127 - val_mean_absolute_error: 0.7127\n",
      "Epoch 2/10\n",
      "224/224 [==============================] - 59s 263ms/step - loss: 0.6719 - mean_absolute_error: 0.6719 - val_loss: 0.6226 - val_mean_absolute_error: 0.6226\n",
      "Epoch 3/10\n",
      "224/224 [==============================] - 59s 261ms/step - loss: 0.6125 - mean_absolute_error: 0.6125 - val_loss: 0.5974 - val_mean_absolute_error: 0.5974\n",
      "Epoch 4/10\n",
      "224/224 [==============================] - 59s 262ms/step - loss: 0.5712 - mean_absolute_error: 0.5712 - val_loss: 0.5868 - val_mean_absolute_error: 0.5868\n",
      "Epoch 5/10\n",
      "224/224 [==============================] - 60s 269ms/step - loss: 0.5518 - mean_absolute_error: 0.5518 - val_loss: 0.6015 - val_mean_absolute_error: 0.6015\n",
      "Epoch 6/10\n",
      "224/224 [==============================] - 60s 268ms/step - loss: 0.5255 - mean_absolute_error: 0.5255 - val_loss: 0.5673 - val_mean_absolute_error: 0.5673\n",
      "Epoch 7/10\n",
      "224/224 [==============================] - 60s 270ms/step - loss: 0.5075 - mean_absolute_error: 0.5075 - val_loss: 0.5753 - val_mean_absolute_error: 0.5753\n",
      "Epoch 8/10\n",
      "224/224 [==============================] - 61s 272ms/step - loss: 0.4842 - mean_absolute_error: 0.4842 - val_loss: 0.5497 - val_mean_absolute_error: 0.5497\n",
      "Epoch 9/10\n",
      "224/224 [==============================] - 61s 272ms/step - loss: 0.4707 - mean_absolute_error: 0.4707 - val_loss: 0.5339 - val_mean_absolute_error: 0.5339\n",
      "Epoch 10/10\n",
      "224/224 [==============================] - 61s 273ms/step - loss: 0.4500 - mean_absolute_error: 0.4500 - val_loss: 0.5345 - val_mean_absolute_error: 0.5345\n"
     ]
    }
   ],
   "source": [
    "history = model_lstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 25s 51ms/step - loss: 0.5340 - mean_absolute_error: 0.5340\n"
     ]
    }
   ],
   "source": [
    "scores_lstm = model_lstm.evaluate(X_test_sequences, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 26s 52ms/step\n",
      "For LSTM MAE = 0.531, Accuracy =0.604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1564,  291,  109,   91,   89],\n",
       "       [ 462,  467,  211,  182,   56],\n",
       "       [ 156,  349,  441,  712,  229],\n",
       "       [  60,  122,  297, 1714, 1468],\n",
       "       [  59,   64,  129, 1059, 5252]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction=model_lstm.predict(X_test_sequences)\n",
    "prediction[prediction<1]=1\n",
    "prediction[prediction>5]=5\n",
    "ACC=accuracy_score(y_test,np.round(prediction))\n",
    "MAE=mean_absolute_error(y_test,prediction)\n",
    "print('For LSTM MAE = {0:.3f}, Accuracy ={1:.3f}'.format(MAE,ACC))\n",
    "display(confusion_matrix(y_test,np.round(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de la sortie des LSTM dans d'autres modèles supervisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.embedding.Embedding at 0x243b9aa7150>,\n",
       " <keras.src.layers.rnn.lstm.LSTM at 0x24308561fd0>,\n",
       " <keras.src.layers.rnn.lstm.LSTM at 0x243b9ad8cd0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2430544c6d0>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "992/992 [==============================] - 52s 52ms/step\n",
      "489/489 [==============================] - 19s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "#Création d'un modele qui prend en entrée les memes données que le modele lstm et qui renvoie les sorties de l'avant derniere couche\n",
    "#C'est à dire les features extraites par le LSTM qui correspondent au contexte de chaque texte\n",
    "Model_rf = Model(inputs=model_lstm.inputs, outputs=model_lstm.layers[2].output)\n",
    "\n",
    "#Sauvegarde du modele\n",
    "Model_rf.save('models/model_lstm_features.h5')\n",
    "\n",
    "#Création des du contexte des données train et test\n",
    "corpus_train_lstm = Model_rf.predict(X_train_sequences)\n",
    "corpus_test_lstm = Model_rf.predict(X_test_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31738, 100)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## RF #############\n",
      "MAE = 0.587, Accuracy =0.564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1089,  676,  214,  127,   38],\n",
       "       [ 220,  572,  377,  185,   24],\n",
       "       [  51,  349,  631,  737,  119],\n",
       "       [  15,  111,  439, 2076, 1020],\n",
       "       [  22,   73,  226, 1789, 4453]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## KNN #############\n",
      "MAE = 0.597, Accuracy =0.565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1163,  606,  198,  117,   60],\n",
       "       [ 283,  503,  358,  194,   40],\n",
       "       [  79,  310,  605,  709,  184],\n",
       "       [  28,  109,  420, 1902, 1202],\n",
       "       [  26,   68,  225, 1586, 4658]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## MLP #############\n",
      "MAE = 0.569, Accuracy =0.588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1198,  553,  219,  117,   57],\n",
       "       [ 242,  520,  388,  195,   33],\n",
       "       [  73,  286,  653,  725,  150],\n",
       "       [  26,   83,  424, 1902, 1226],\n",
       "       [  24,   71,  187, 1364, 4917]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Execution des algorithmes de prediction sur les features extraites par le LSTM\n",
    "run_models(corpus_train_lstm,y_train,corpus_test_lstm,y_test,algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Bidirectionnal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 100)         2263000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 600)         962400    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 200)               560800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3786401 (14.44 MB)\n",
      "Trainable params: 1523401 (5.81 MB)\n",
      "Non-trainable params: 2263000 (8.63 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "# Déclaration de l'architecture du modèle\n",
    "model_bilstm = Sequential()\n",
    "\n",
    "# Ajout de la couche d'embedding en entrée du modèle\n",
    "model_bilstm.add(Embedding(len(model.wv.key_to_index) + 1,\n",
    "                            EMBEDDINGS_LEN,  # Embedding size\n",
    "                            weights=[embeddings_index],\n",
    "                            trainable=False))\n",
    "# Ajout de la couche BiLSTM avec une sortie\n",
    "model_bilstm.add(Bidirectional(LSTM(300, dropout=0.2, return_sequences=True)))\n",
    "\n",
    "# Ajout de la couche BiLSTM avec sans sortie\n",
    "model_bilstm.add(Bidirectional(LSTM(100, dropout=0.2)))\n",
    "\n",
    "# Ajout de la couche de sortie avec un neurone\n",
    "model_bilstm.add(Dense(1))\n",
    "model_bilstm.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "224/224 [==============================] - 124s 515ms/step - loss: 0.8719 - mean_absolute_error: 0.8719 - val_loss: 0.7490 - val_mean_absolute_error: 0.7490\n",
      "Epoch 2/10\n",
      "224/224 [==============================] - 121s 539ms/step - loss: 0.6797 - mean_absolute_error: 0.6797 - val_loss: 0.6496 - val_mean_absolute_error: 0.6496\n",
      "Epoch 3/10\n",
      "224/224 [==============================] - 121s 540ms/step - loss: 0.6288 - mean_absolute_error: 0.6288 - val_loss: 0.6103 - val_mean_absolute_error: 0.6103\n",
      "Epoch 4/10\n",
      "224/224 [==============================] - 124s 553ms/step - loss: 0.5966 - mean_absolute_error: 0.5966 - val_loss: 0.5957 - val_mean_absolute_error: 0.5957\n",
      "Epoch 5/10\n",
      "224/224 [==============================] - 132s 591ms/step - loss: 0.5534 - mean_absolute_error: 0.5534 - val_loss: 0.5925 - val_mean_absolute_error: 0.5925\n",
      "Epoch 6/10\n",
      "224/224 [==============================] - 116s 519ms/step - loss: 0.5270 - mean_absolute_error: 0.5270 - val_loss: 0.6090 - val_mean_absolute_error: 0.6090\n",
      "Epoch 7/10\n",
      "224/224 [==============================] - 114s 511ms/step - loss: 0.4996 - mean_absolute_error: 0.4996 - val_loss: 0.5672 - val_mean_absolute_error: 0.5672\n",
      "Epoch 8/10\n",
      "224/224 [==============================] - 106s 474ms/step - loss: 0.4867 - mean_absolute_error: 0.4867 - val_loss: 0.5407 - val_mean_absolute_error: 0.5407\n",
      "Epoch 9/10\n",
      "224/224 [==============================] - 114s 511ms/step - loss: 0.4681 - mean_absolute_error: 0.4681 - val_loss: 0.5579 - val_mean_absolute_error: 0.5579\n",
      "Epoch 10/10\n",
      "224/224 [==============================] - 115s 514ms/step - loss: 0.4506 - mean_absolute_error: 0.4506 - val_loss: 0.5447 - val_mean_absolute_error: 0.5447\n"
     ]
    }
   ],
   "source": [
    "history = model_bilstm.fit(X_train_sequences, y_train, epochs=10, batch_size=128, verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 25s 50ms/step - loss: 0.5345 - mean_absolute_error: 0.5345\n"
     ]
    }
   ],
   "source": [
    "scores_bilstm = model_bilstm.evaluate(X_test_sequences, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992/992 [==============================] - 48s 46ms/step\n",
      "489/489 [==============================] - 21s 44ms/step\n"
     ]
    }
   ],
   "source": [
    "#Création d'un modele qui prend en entrée les memes données que le modele bilstm et qui renvoie les sorties de l'avant derniere couche\n",
    "#C'est à dire les features extraites par le biLSTM qui correspondent au contexte de chaque texte\n",
    "Model_rf = Model(inputs=model_bilstm.inputs, outputs=model_bilstm.layers[2].output)\n",
    "\n",
    "#Sauvegarde du modele\n",
    "Model_rf.save('models/model_bilstm_features.h5')\n",
    "\n",
    "#Création des du contexte des données train et test\n",
    "corpus_train_bilstm = Model_rf.predict(X_train_sequences)\n",
    "corpus_test_bilstm = Model_rf.predict(X_test_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## RF #############\n",
      "MAE = 0.575, Accuracy =0.575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1166,  620,  202,  127,   29],\n",
       "       [ 236,  525,  407,  194,   16],\n",
       "       [  56,  322,  685,  728,   96],\n",
       "       [  17,  116,  464, 2035, 1029],\n",
       "       [  19,   62,  226, 1671, 4585]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## KNN #############\n",
      "MAE = 0.585, Accuracy =0.576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1291,  458,  197,  132,   66],\n",
       "       [ 305,  431,  379,  222,   41],\n",
       "       [  95,  270,  598,  742,  182],\n",
       "       [  32,   96,  386, 1861, 1286],\n",
       "       [  31,   63,  166, 1478, 4825]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## MLP #############\n",
      "MAE = 0.589, Accuracy =0.586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1301,  443,  199,  157,   44],\n",
       "       [ 294,  450,  392,  207,   35],\n",
       "       [ 111,  271,  617,  758,  130],\n",
       "       [  34,  110,  418, 1897, 1202],\n",
       "       [  39,   58,  166, 1399, 4901]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(corpus_train_bilstm,y_train,corpus_test_bilstm,y_test,algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultat obtenus par le BiLSTM sont sensiblement les mémes qu'avec les LSTM "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
